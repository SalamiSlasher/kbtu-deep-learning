{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df= pd.DataFrame()\n",
    "\n",
    "df = pd.read_csv('texts_train.txt', sep=\"\\t\", encoding='UTF-8',header=0)\n",
    "df['grade'] = pd.read_csv('scores_train.txt', sep=\"\\t\", encoding='UTF-8',header=0)\n",
    "df.columns = ['comment', 'grade']\n",
    "df.sample(5)"
   ],
   "id": "d067a88748331318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "unique_grades = np.sort(df['grade'].unique())\n",
    "print('all unique grades: ',unique_grades, type(unique_grades))"
   ],
   "id": "63022448663fced5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_new = pd.DataFrame()\n",
    "comment = []\n",
    "grade = []\n",
    "\n",
    "for grades in unique_grades:\n",
    "    counter = 0\n",
    "    for i in range(len(df)):\n",
    "        #for i in range(000/(len(unique_grades))):\n",
    "        if (counter<300):\n",
    "            if (grades == df['grade'][i]):\n",
    "                comment.append(df['comment'][i])\n",
    "                grade.append(df['grade'][i])\n",
    "                counter = counter + 1\n",
    "df_new['comment'] = comment\n",
    "#Заменим значения оценок на 1,2,3\n",
    "#Это облегчит подсчет ошибки\n",
    "#1 - негативный отзыв\n",
    "#2 - нейтральный\n",
    "#3 - положительный\n",
    "df_new['grade'] = grade\n",
    "df_new['grade'] = df_new['grade'].replace(1, 0).replace(2, 0).replace(3, 0)\n",
    "df_new['grade'] = df_new['grade'].replace(4, 1).replace(5, 1).replace(6, 1).replace(7, 1)\n",
    "df_new['grade'] = df_new['grade'].replace(10, 2).replace(9, 2).replace(8, 2)\n",
    "print('new unique grades: ',np.sort(df_new['grade'].unique()))\n",
    "df_new.head"
   ],
   "id": "59fb98571a62973b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "array_of_mean_thensors = []\n",
    "grade = []\n",
    "comment = []\n",
    "uncomtiled_rows = 0\n",
    "for i in range(len(df_new)):\n",
    "    try:\n",
    "        grade.append(df_new['grade'][i])\n",
    "        comment.append(df_new['comment'][i])\n",
    "    except:\n",
    "        #in some cases, from my point of view, when comment is too large, the data has not processed, and it cause some \n",
    "        print('wtf')\n",
    "        uncomtiled_rows = uncomtiled_rows + 1\n",
    "        #print('Too long or to short comment')\n",
    "print(uncomtiled_rows)\n",
    "print(comment[0],' ',grade[0])"
   ],
   "id": "43d3dcf311e75cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from string import punctuation\n",
    "print(punctuation)\n",
    "def preproccessing(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    text = text.split(' ')\n",
    "    #print ('Number of reviews :', len(text))\n",
    "    return text\n",
    "tokenized_corpus = [preproccessing(i) for i in comment ]\n",
    "tokenized_corpus[0]"
   ],
   "id": "c50301c7b08118dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)} # word -> index\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)} # index -> word\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary)"
   ],
   "id": "6528a3cced3f575",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:35.873659Z"
    }
   },
   "cell_type": "code",
   "source": "len(vocabulary), len(word2idx), len(idx2word)",
   "id": "adc4bd37639dafdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:35.877102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Заменили слова в предложении на токены (str -> int)\n",
    "\n",
    "comment_in_num = []\n",
    "for sentence in tokenized_corpus:\n",
    "    comment_in_num.append([word2idx[word] for word in sentence])\n",
    "comment_in_num[6]\n",
    "#tokenized_corpus[6]"
   ],
   "id": "91fc562e293f45bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:35.894287Z"
    }
   },
   "cell_type": "code",
   "source": "len(comment_in_num) == len(tokenized_corpus) == len(grade)",
   "id": "9d79b1cf7e64b36b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:35.914976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "comment_len = [len(x) for x in comment_in_num]\n",
    "pd.Series(comment_len).hist()\n",
    "plt.show()\n",
    "pd.Series(comment_len).describe()"
   ],
   "id": "af583d4f724c157a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:36.009138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  x, y -> матрица размерностью кол-во отзывов X желаемая длина предложений\n",
    "def pad_features(reviews_int, seq_length):\n",
    "    #Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "\n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "\n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "\n",
    "        features[i,:] = np.array(new)\n",
    "\n",
    "    return features"
   ],
   "id": "b6487e85aec3745",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:36.011812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = pad_features(comment_in_num, 200) # why 200???\n",
    "features"
   ],
   "id": "36090c8c9115a09d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:36.074415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(features),np.array(grade), test_size=0.20, random_state=42)\n",
    "train_data = TensorDataset(torch.cuda.FloatTensor(X_train), torch.cuda.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.cuda.FloatTensor(X_test), torch.cuda.LongTensor(y_test))\n",
    "# dataloaders\n",
    "batch_size = 30\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ],
   "id": "d43b8f8a2bd1bfa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:36.121853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ],
   "id": "554f4198bccca59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:36.121853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden"
   ],
   "id": "8be78127e28a9adc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:36.122853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocabulary)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ],
   "id": "897f99192a97841b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T08:21:21.554797Z",
     "start_time": "2024-04-13T08:21:21.523967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# training params\n",
    "\n",
    "\n",
    "counter = 0\n",
    "print_every = 10\n",
    "clip=5 # gradient clipping\n",
    "# move model to GPU, if available\n",
    "train_on_gpu = True\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "    pass\n",
    "\n",
    "net.train()\n",
    "losses = []\n",
    "epochs = 5\n",
    "# train for some number of epochs\n",
    "for e in tqdm.tqdm (range(epochs)):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    loss_ = []\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        net.train()\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.cuda.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        loss_.append(loss.item())\n",
    "    losses.append(np.mean(loss_))"
   ],
   "id": "d7cda07e50e43779",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m train_on_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(train_on_gpu):\n\u001B[1;32m---> 16\u001B[0m     \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m     19\u001B[0m net\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32mE:\\edu\\deep-learning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:911\u001B[0m, in \u001B[0;36mModule.cuda\u001B[1;34m(self, device)\u001B[0m\n\u001B[0;32m    894\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m    895\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[0;32m    896\u001B[0m \n\u001B[0;32m    897\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    909\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[0;32m    910\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 911\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\edu\\deep-learning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 802\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    806\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    807\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    812\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    813\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mE:\\edu\\deep-learning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:221\u001B[0m, in \u001B[0;36mRNNBase._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    216\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m_apply(fn, recurse)\n\u001B[0;32m    218\u001B[0m \u001B[38;5;66;03m# Resets _flat_weights\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001B[39;00m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001B[39;00m\n\u001B[1;32m--> 221\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_flat_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[1;32mE:\\edu\\deep-learning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:158\u001B[0m, in \u001B[0;36mRNNBase._init_flat_weights\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, wn) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, wn) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    155\u001B[0m                       \u001B[38;5;28;01mfor\u001B[39;00m wn \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights_names]\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weight_refs \u001B[38;5;241m=\u001B[39m [weakref\u001B[38;5;241m.\u001B[39mref(w) \u001B[38;5;28;01mif\u001B[39;00m w \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    157\u001B[0m                           \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights]\n\u001B[1;32m--> 158\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\edu\\deep-learning\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:209\u001B[0m, in \u001B[0;36mRNNBase.flatten_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    208\u001B[0m     num_weights \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 209\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cudnn_rnn_flatten_weight\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cudnn_mode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproj_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T08:19:36.123853Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "839855225a492d5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T08:19:36.134365Z",
     "start_time": "2024-04-13T08:19:36.123853Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3240b1f8ec680613",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
